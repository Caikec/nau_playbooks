# {{ ansible_managed }}

# This is the base file for the fluent bit.
# It overrides the default file on the container image, file link:
#   https://github.com/fluent/fluent-bit/blob/master/conf/fluent-bit.conf

[SERVICE]
    # Interval to flush output in seconds
    Flush            30
    Daemon           Off

    # Load parsers definition files
    # load the default parsers file, link: https://github.com/fluent/fluent-bit/blob/master/conf/parsers.conf
    Parsers_File     parsers.conf
    # load the extra custom business parsers
    Parsers_File     extra-parsers.conf

    Streams_File     stream_processor.conf

    # Store streams and chunks of data in the filesystem.
    # If this parameter is not set, Input plugins can only use in-memory buffering.
    # So we don't lose any data if the fluentbit container needs to restart
    storage.path     /fluent-bit/data/storage

    # Next lines for test proposes.
    # To view the number of metrics processed run on each input, parser and output run the next cmd:
    #   curl -s http://127.0.0.1:2020/api/v1/metrics/ | jq<
    Log_Level        debug # info
    HTTP_Server      On
    # HTTP_Listen      0.0.0.0
    HTTP_Port        2020

# Forward is the protocol used by Fluent Bit and Fluentd to route messages between peers. 
# This plugin implements the input service to listen for Forward messages.
# This fluent bit instance would receive all messages from the fluent bit relay instances.
# 
# The default docker daemon logging driver is the fluentd and the drive is configured
# in a way that each log message is tagged with the prefix `docker.container.log.` and then
# the docker container name. So the final `Tag` would be something like:
# `docker.container.log.<stack>_<service>.<slot>.<id>`.
[INPUT]
    Name              forward
    Listen            0.0.0.0
    Port              {{ observability_fluentbit_aggregator_forward_port }}
    Buffer_Chunk_Size 1M
    Buffer_Max_Size   6M
    Alias             docker_container_log

# Add `docker_stack` and `docker_service` new records from forward log messages from docker 
# container logs using the tag it self.
[FILTER]
    Name            parser
    Match           docker.container.log.*
    Key_Name        container_name
    Parser          docker_container_name_to_stack_and_service
# Keep all other original fields in the parsed result
    Reserve_Data    On
# Keep original Key_Name field in the parsed result
    Preserve_Key    On
    Alias           docker_container_name

# Use a fluentbit filter to use the Parser Filter plugin that allows to parse the `log` field
# in event records. 
# It uses the `openedx_tracking_logs_parser` parser defined as an extra/custom parser defined in
# `extra-parsers.conf` file to log messages for the openedx lms and cms containers.
# It extracts the `tracking_json` field from `log` field.
[FILTER]
    Name            parser
    Match_Regex     docker.container.log.openedx_(l|c)ms
    Key_Name        log
    Parser          openedx_tracking_logs_parser
# Keep all other original fields in the parsed result
    Reserve_Data    On
# Keep original Key_Name field in the parsed result
    Preserve_Key    On
    Alias           openedx_tracking_logs

# Parses the `tracking_json` field extracted from the previous `openedx_tracking_logs` filter
# and interprets it has json making available each openedx tracking field has another fluent bit
# field.
[FILTER]
    Name             parser
    Match            docker.container.log.openedx_(l|c)ms
    Key_Name         tracking_json
    Parser           parse_as_json
# Keep original Key_Name field in the parsed result
    Preserve_Key     On
    Reserve_Data     True

# Split richie nginx log message to multiple attributes
[FILTER]
    Name             parser
    Match            docker.container.log.staticproxy_nginx
    Key_Name         log
    Parser           staticproxy_nginx_log_parser
# Keep original Key_Name field in the parsed result
    Preserve_Key     On
    Reserve_Data     True

# Split richie nginx log message to multiple attributes
[FILTER]
    Name             parser
    Match            docker.container.log.coursecertificate_nginx
    Key_Name         log
    Parser           coursecertificate_nginx_log_parser
# Keep original Key_Name field in the parsed result
    Preserve_Key     On
    Reserve_Data     True

# Split richie nginx log message to multiple attributes
[FILTER]
    Name             parser
    Match            docker.container.log.richie_nau_nginx
    Key_Name         log
    Parser           richie_nginx_log_parser
# Keep original Key_Name field in the parsed result
    Preserve_Key     On
    Reserve_Data     True

# Split openedx nginx log message
[FILTER]
    Name             parser
    Match            docker.container.log.openedx_nginx
    Key_Name         log
    Parser           openedx_nginx_log_parser
# Keep original Key_Name field in the parsed result
    Preserve_Key     On
    Reserve_Data     True

# Send to Ceph S3 the openedx tracking logs that has the tracking_json attribute
[OUTPUT]
    Name            s3
    Match           openedx.trackinglogs
    bucket          {{ COMMON_OBJECT_STORE_LOG_SYNC_BUCKET }}
# get a file per hour per fluentbit instance, one per host
    upload_timeout  60m 
# the path and the file name on the S3 bucket
    s3_key_format   /trackinglog/${DOCKER_NODE_HOSTNAME}/tracking.log-%Y%m%d-%H%M%S-$UUID.gz
# only the value of that key will be sent to S3
    log_key         tracking_json
# The ceph URL
    preserve_data_ordering True
# Use this folder to locally buffer data before sending. The parent folder is a docker volume so
# it is maintained on fluentbit's docker restart.
    store_dir       /fluent-bit/data/s3/openedx_trackinglogs
    endpoint        https://{{ nau_ceph_host }}
    Alias           openedx_trackinglogs_ceph_s3
    
    # tls.verify      false
    # region          somewhere
    # use_put_object  On
    # workers         0
    # tls.vhost       {{ nau_ceph_host }}

{% for docker_service in observability_fluentbit_output_docker_service_s3 %}
# Send to Ceph S3 the docker service `{{ docker_service }}` logs
[OUTPUT]
    Alias           {{ docker_service }}_ceph_s3
    Name            s3
    Match           docker.container.log.{{ docker_service }}
    bucket          {{ COMMON_OBJECT_STORE_LOG_SYNC_BUCKET }}
# get a file per hour per fluentbit instance, one per host
    upload_timeout  60m
# the path and the file name on the S3 bucket
    s3_key_format   /trackinglog/docker_service/{{ docker_service }}/logs_{{ docker_service }}_%Y%m%d-%H%M%S-$UUID.gz
# only the value of that key will be sent to S3
    log_key         log
# The ceph URL
    preserve_data_ordering True
# Use this folder to locally buffer data before sending. The parent folder is a docker volume so
# it is maintained on fluentbit's docker restart.
    store_dir       /fluent-bit/data/s3/{{ docker_service }}
    endpoint        https://{{ nau_ceph_host }}

{% endfor %}

[OUTPUT]
    Name            file
    Match           metrics.*
    Path            /fluent-bit/data/output-files/
    Mkdir           True
    Format          csv

# Allow to view the messages using docker logs, only for test purposes.
[OUTPUT]
    Name            stdout
    Match           *
    # Match           openedx_lms*
# view only the openedx lms and cms container logs.
    # Match_Regex     openedx_(l|c)ms.*
    Alias raw_output
