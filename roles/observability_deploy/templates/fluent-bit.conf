# {{ ansible_managed }}

# This is the base file for the fluent bit.
# It overrides the default file on the container image, file link:
#   https://github.com/fluent/fluent-bit/blob/master/conf/fluent-bit.conf

[SERVICE]
    # Interval to flush output in seconds
    Flush            30
    Daemon           Off

    # Load parsers definition files
    # load the default parsers file, link: https://github.com/fluent/fluent-bit/blob/master/conf/parsers.conf
    Parsers_File     parsers.conf
    # load the extra custom business parsers
    Parsers_File     extra-parsers.conf

    Streams_File     stream_processor.conf

    # Store streams and chunks of data in the filesystem.
    # If this parameter is not set, Input plugins can only use in-memory buffering.
    # So we don't lose any data if the fluentbit container needs to restart
    storage.path     /fluent-bit/data/storage

    # Next lines for test proposes.
    # To view the number of metrics processed run on each input, parser and output run the next cmd:
    #   curl -s http://127.0.0.1:2020/api/v1/metrics/ | jq<
    Log_Level        debug # info
    HTTP_Server      On
    # HTTP_Listen      0.0.0.0
    HTTP_Port        2020

# Tail multiple docker host operating system log files and use a specific parser to proccess them.
[INPUT]
    Name           Tail
    Path           /var/log/auth.log
    Tag            auth
    Path_Key       log_file
    Parser         syslog-rfc3164
    Alias          log_auth

[INPUT]
    Name           Tail
    Path           /var/log/syslog
    Tag            syslog
    Path_Key       log_file
    Parser         syslog-rfc3164
    Alias          log_syslog

[INPUT]
    Name           Tail
    Path           /var/log/dpkg.log
    Tag            dpkg
    Path_Key       log_file
    Parser         syslog-rfc3164
    Alias          log_dpkg

[INPUT]
    Name           Tail
    Path           /var/log/mail.log
    Tag            mail
    Path_Key       log_file
    Parser         syslog-rfc3164
    Alias          log_mail

# Forward is the protocol used by Fluent Bit and Fluentd to route messages between peers. 
# This plugin implements the input service to listen for Forward messages.
# This input prefix the `Tag` with the prefix `docker.container.log.`.
# The default docker daemon logging driver is the fluentd and the drive is configured
# in a way that each log message is tagged with the docker container name.
# So the final `Tag` would be something like `docker.container.log.<stack>_<service>.<slot>.<id>`.
[INPUT]
    Name              forward
    Listen            0.0.0.0
    Port              24224
    Buffer_Chunk_Size 1M
    Buffer_Max_Size   6M
    Tag_Prefix        docker.container.log.
    Alias             docker_container_log

# The docker input plugin allows you to collect Docker container metrics such as memory usage and
# CPU consumption.
# [INPUT]
#     Name           docker
#     Tag            docker.metric

# The docker events input plugin uses the docker API to capture server events. 
# A complete list of possible events returned by this plugin can be found on
# https://docs.docker.com/engine/reference/commandline/events/
# [INPUT]
#     Name           docker_events
#     Tag            docker.event

# Prometheus Node Exporter is a popular way to collect system level metrics from 
# operating systems, such as CPU / Disk / Network / Process statistics. 
# Fluent Bit 1.8.0 includes node exporter metrics plugin that builds off the
# Prometheus design to collect system level metrics without having to manage
# two separate processes or agents.
# [INPUT]
#     Name            node_exporter_metrics
#     Tag             node_metrics
# # every 60 seconds
#     Scrape_interval 60
# #     # Inspired by:  https://github.com/fluent/fluent-bit/blob/master/docker_compose/node-exporter-dashboard/docker-compose.yml
#     Path.procfs     /host/proc
#     Path.sysfs      /host/sys
#     Alias           node_metrics

# Add the docker node hostname to the `host_name` new record from the environment variable 
# `DOCKER_NODE_HOSTNAME` to all the logs.
[FILTER]
    Name            record_modifier
    Match           *
    Record          host_name ${DOCKER_NODE_HOSTNAME}
    Alias           host_name

# Add `docker_stack` and `docker_service` new records from forward log messages from docker 
# container logs using the tag it self.
# [FILTER]
#     name            record_modifier
#     Match           docker.container.log.*
#     Record          docker_stack $TAG[3]
#     Record          docker_service $TAG[4]
#     Alias           docker_stack_and_service
[FILTER]
    Name            parser
    Match           docker.container.log.*
    Key_Name        container_name
    Parser          docker_container_name_to_stack_and_service
# Keep all other original fields in the parsed result
    Reserve_Data    On
# Keep original Key_Name field in the parsed result
    Preserve_Key    On
    Alias           docker_container_name

# Use a fluentbit filter to use the Parser Filter plugin that allows to parse the `log` field
# in event records. 
# It uses the `openedx_tracking_logs_parser` parser defined as an extra/custom parser defined in
# `extra-parsers.conf` file to log messages for the openedx lms and cms containers.
# It extracts the `tracking_json` field from `log` field.
[FILTER]
    Name            parser
    Match_Regex     docker.container.log.openedx_(l|c)ms
    Key_Name        log
    Parser          openedx_tracking_logs_parser
# Keep all other original fields in the parsed result
    Reserve_Data    On
# Keep original Key_Name field in the parsed result
    Preserve_Key    On
    Alias           openedx_tracking_logs

# Parses the `tracking_json` field extracted from the previous `openedx_tracking_logs` filter
# and interprets it has json making available each openedx tracking field has another fluent bit
# field.
[FILTER]
    Name             parser
    Match            docker.container.log.openedx_(l|c)ms
    Key_Name         tracking_json
    Parser           parse_as_json
# Keep original Key_Name field in the parsed result
    Preserve_Key    On
    Reserve_Data     True

# Send to Ceph S3 the openedx tracking logs
[OUTPUT]
    Name            s3
    Match_Regex     docker.container.log.openedx_(l|c)ms
    bucket          {{ COMMON_OBJECT_STORE_LOG_SYNC_BUCKET }}
# get a file per hour per fluentbit instance, one per host
    upload_timeout  60m 
# the path and the file name on the S3 bucket
    s3_key_format   /trackinglog/${DOCKER_NODE_HOSTNAME}/tracking.log-%Y%m%d-%H%M%S-$UUID.gz
# only the value of that key will be sent to S3
    log_key         tracking_json
# The ceph URL
    preserve_data_ordering True
# Use this folder to locally buffer data before sending. The parent folder is a docker volume so
# it is maintained on fluentbit's docker restart.
    store_dir       /fluent-bit/data/s3
    endpoint        https://{{ nau_ceph_host }}
    Alias           ceph_s3
    
    # tls.verify      false
    # region          somewhere
    # use_put_object  On
    # workers         0
    # tls.vhost       {{ nau_ceph_host }}


# Allow to view the messages using docker logs, only for test purposes.
[OUTPUT]
    Name            stdout
    Match           *
    # Match           openedx_lms*
# view only the openedx lms and cms container logs.
    # Match_Regex     openedx_(l|c)ms.*
    Alias raw_output
