{% import '_docker_deploy_helper.j2' as helper with context %}
# {{ ansible_managed }}
version: "3.8"

services:

  # TODO: add "mongo" cluster

{% if openedx_insights %}
  insights:
    image: {{ openedx_insights_image }}
    environment:
{% for env_var_name, env_var_value in openedx_insights_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ openedx_insights_replicas }}
      update_config:
        # We are updating 1 container a time
        parallelism: 1
        parallelism: {{ [ 1, ( openedx_insights_replicas / 2 ) ] | max | round(0, 'floor') | int }}
      placement:
        max_replicas_per_node: {{ ( openedx_insights_replicas / docker_swarm_cluster_group_length | int ) | round(0, 'ceil') | int }}
{% if openedx_insights_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_insights == true
{% endif %}
      resources:
        limits:
          memory: 500M
          cpus: "{{ openedx_insights_limit_cpus }}"
        reservations:
          memory: 200M
    healthcheck:
      # Check if /health/ returns a HTTP status 200, meaning the service is ok
      test: curl --silent --fail --head --resolve insights:8000:127.0.0.1 http://insights:8000/health/ | egrep "HTTP/.*200 OK"
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s
{{ helper.service_configs(service='insights') }}
{{ helper.service_secrets(service='insights') }}

{% if openedx_insights_job %}
  insights-job:
    image: {{ openedx_insights_image }}
    command: {{ openedx_insights_job_command }}
    environment:
{% for env_var_name, env_var_value in openedx_insights_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: 1 # should exist only 1 instance of this container
      mode: replicated # after docker 22.06.0 release change it to replicated-job
      restart_policy:
        condition: on-failure # restart this container only if it fails to run
      placement:
{% if openedx_insights_job_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_insights_job == true
{% endif %}
      resources:
        limits:
          cpus: "{{ openedx_insights_limit_cpus }}"
{{ helper.service_configs(service='insights') }}
{{ helper.service_secrets(service='insights') }}

{% endif %}{# comment if openedx_insights_job #}
{% endif %}{# comment if openedx_insights #}

{% if openedx_analyticsapi %}
  analyticsapi:
    image: {{ openedx_analyticsapi_image }}
    environment:
{% for env_var_name, env_var_value in openedx_analyticsapi_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    networks:
      default:
        aliases:
          # alias configured on discovery user interface https://discovery.<environment>/admin/core/partner/1/change/
          - openedx.analyticsapi
    deploy:
      replicas: {{ openedx_analyticsapi_replicas }}
      update_config:
        # We are updating 1 container a time
        parallelism: {{ openedx_analyticsapi_deploy_parallelism }}
      placement:
        max_replicas_per_node: {{ ( openedx_analyticsapi_replicas / docker_swarm_cluster_group_length | int ) | round(0, 'ceil') | int }}
{% if openedx_analyticsapi_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_analyticsapi == true
{% endif %}
      resources:
        limits:
          memory: 250M
          cpus: "{{ openedx_analyticsapi_limit_cpus }}"
        reservations:
          memory: 150M
    healthcheck:
      # Check if /health/ returns a HTTP status 200, meaning the service is ok
      test: curl --silent --fail --head --resolve analyticsapi:8100:127.0.0.1 http://analyticsapi:8100/health/ | egrep "HTTP/.*200 OK"
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s
{{ helper.service_configs(service='analyticsapi') }}
{{ helper.service_secrets(service='analyticsapi') }}

{% if openedx_analyticsapi_job %}
  analyticsapi-job:
    image: {{ openedx_analyticsapi_image }}
    command: {{ openedx_analyticsapi_job_command }}
    environment:
{% for env_var_name, env_var_value in openedx_analyticsapi_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: 1 # should exist only 1 instance of this container
      mode: replicated # after docker 22.06.0 release change it to replicated-job
      restart_policy:
        condition: on-failure # restart this container only if it fails to run
      placement:
{% if openedx_analyticsapi_job_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_analyticsapi_job == true
{% endif %}
      resources:
        limits:
          cpus: "{{ openedx_analyticsapi_limit_cpus }}"
{{ helper.service_configs(service='analyticsapi') }}
{{ helper.service_secrets(service='analyticsapi') }}

{% endif %}{# comment if openedx_analyticsapi_job #}
{% endif %}{# comment if openedx_analyticsapi #}
  nginx:
    image: {{ openedx_nginx_image }}
    ports:
      - target:    {{ openedx_nginx_http }}
        published: {{ openedx_nginx_http_ingress_port }}
        protocol:  tcp
        mode:      ingress
      - target:    {{ openedx_nginx_https }}
        published: {{ openedx_nginx_https_ingress_port }}
        protocol:  tcp
        mode:      ingress
    deploy:
      replicas: {{ openedx_nginx_replicas }}
      update_config:
        # use the default 'stop-first' so we can detect on replicas health check that something is still not converged.
        order: stop-first
        # rolling deploy with half the replicas, if replicas has an odd number, when round to a lower number,
        # so in the limit we have 3 update batches, 2 bigger half batches and +1 at the end.
        # Example:
        #    #replicas -> #parallelism
        #    2 --> 1
        #    3 --> 1
        #    4 --> 2
        #    5 --> 2
        parallelism: {{ openedx_nginx_deploy_parallelism }}
      placement:
        max_replicas_per_node: {{ ( openedx_nginx_replicas / docker_swarm_cluster_group_length | int ) | round(0, 'ceil') | int }}
{% if openedx_nginx_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_nginx == true
{% endif %}
      resources:
        limits:
          memory: 128M
          cpus: "{{ openedx_nginx_limit_cpus }}"
        reservations:
          memory: 50M
    extra_hosts:
      - "{{ nau_ceph_host }}:{{ nau_ceph_s3_ip }}"
    healthcheck:
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/80", "&&", "echo", "''", ">", "/dev/tcp/127.0.0.1/443", "||", "exit", "1"]
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s
{{ helper.service_configs(service='nginx') }}
{{ helper.service_secrets(service='nginx') }}

  lms:
    image: {{ openedx_app_image }}
    environment:
{% for env_var_name, env_var_value in openedx_lms_env_var.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ openedx_lms_replicas }}
      update_config:
        parallelism: {{ openedx_lms_deploy_parallelism }}
      placement:
        max_replicas_per_node: {{ ( openedx_lms_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
{% if openedx_lms_deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_lms == true
{% endif %}
      resources:
        limits:
          # Tutor in docs: "each worker requires upwards of 500 Mb of RAM" so we give 1G per uwsgi worker
          memory: {{ openedx_lms_uwsgi_workers * 1000 }}M
          cpus: "{{ openedx_lms_limit_cpus }}"
        reservations:
          memory: {{ openedx_lms_uwsgi_workers * 500 }}M
    extra_hosts:
      - "{{ nau_ceph_host }}:{{ nau_ceph_s3_ip }}"    
    healthcheck:
      # Check if heartbeat is ok
      test: curl --silent --fail --head --resolve lms:8000:127.0.0.1 http://lms:8000/heartbeat | egrep "HTTP/.*200 OK"
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s
{{ helper.service_configs(service='edxapp') }}
{{ helper.service_secrets(service='edxapp') }}

{% if openedx_app_job %}
  app-job:
    image: {{ openedx_app_image }}
    environment:
{% for env_var_name, env_var_value in openedx_app_env_var.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    command: {{ openedx_app_job_command }}
    extra_hosts:
      - "{{ nau_ceph_host }}:{{ nau_ceph_s3_ip }}"
    deploy:
      replicas: 1 # should exist only 1 instance of this container
      mode: replicated # after docker 22.06.0 release change it to replicated-job
      restart_policy:
        condition: on-failure # restart this container only if it fails to run
      placement:
{% if openedx_app_job_deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_app_job == true
{% endif %}
      resources:
        limits:
          cpus: "{{ openedx_lms_limit_cpus }}"
{{ helper.service_configs(service='edxapp') }}
{{ helper.service_secrets(service='edxapp') }}
{% endif %}

  cms:
    image: {{ openedx_app_image }}
    environment:
{% for env_var_name, env_var_value in openedx_cms_env_var.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    extra_hosts:
      - "{{ nau_ceph_host }}:{{ nau_ceph_s3_ip }}"
    deploy:
      replicas: {{ openedx_cms_replicas }}
      update_config:
        parallelism: {{ openedx_cms_deploy_parallelism }}
      placement:
        max_replicas_per_node: {{ ( openedx_cms_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
{% if openedx_cms_deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_cms == true
{% endif %}
      resources:
        limits:
          # Tutor in docs: "each worker requires upwards of 500 Mb of RAM" so we give 1G per uwsgi worker
          memory: {{ openedx_cms_uwsgi_workers * 1000 }}M
          cpus: "{{ openedx_cms_limit_cpus }}"
        reservations:
          memory: {{ openedx_cms_uwsgi_workers * 500 }}M
    healthcheck:
      # Check if heartbeat is ok
      test: curl --silent --fail --head --resolve cms:8000:127.0.0.1 http://cms:8000/heartbeat | egrep "HTTP/.*200 OK"
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s
{{ helper.service_configs(service='edxapp') }}
{{ helper.service_secrets(service='edxapp') }}

{% for worker in openedx_celery_workers %}
  {{ worker.service_variant }}-worker-{{ worker.queue }}:
    image: {{ worker.docker_image }}
    environment:
{% for env_var_name, env_var_value in worker.environment_variables_dict.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    command: celery --app={{ worker.service_variant }}.celery worker --loglevel=info --queues=edx.{{ worker.service_variant }}.core.{{ worker.queue }} --hostname=edx.{{ worker.service_variant }}.core.{{ worker.queue }}.%%h --max-tasks-per-child=100 --concurrency={{ worker.processes }}
    extra_hosts:
      - "{{ nau_ceph_host }}:{{ nau_ceph_s3_ip }}"
    deploy:
      replicas: {{ worker.deploy_replicas }}
      placement:
        max_replicas_per_node: {{ worker.deploy_max_replicas_per_node }}
{% if worker.deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.{{ worker.deploy_constraint_label }} == true
{% endif %}
      resources:
        limits:
          memory: {{ worker.deploy_resources_limit_memory }}
          cpus: "{{ worker.deploy_resources_limit_cpus }}"
        reservations:
          memory: {{ worker.deploy_resources_reservations_memory }}
    healthcheck:
      test: celery --app={{ worker.service_variant }}.celery inspect ping -d celery@edx.{{ worker.service_variant }}.core.{{ worker.queue }}.%$$HOSTNAME
      start_period: 120s
      retries: 10
      interval: 120s
      timeout: 90s
{{ helper.service_configs(service='edxapp') }}
{{ helper.service_secrets(service='edxapp') }}

{% endfor %}

  forum:
    image: {{ openedx_forum_docker_image }}
    entrypoint: {{ openedx_forum_docker_entrypoint }}
    environment:
{% for env_var_name, env_var_value in openedx_forum_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ openedx_forum_replicas }}
      update_config:
        parallelism: {{ openedx_forum_deploy_parallelism }}
      placement:
        max_replicas_per_node: {{ ( openedx_forum_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
{% if openedx_forum_deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_forum == true
{% endif %}
      resources:
        limits:
          memory: 500M
          cpus: "{{ openedx_forum_limit_cpus }}"
        reservations:
          memory: 300M
    healthcheck:
      # Check if forum heartbeat returns ok
      test: wget -qSO- 127.0.0.1:{{ openedx_forum_port }}/heartbeat 2>&1 | egrep "HTTP/.*200 OK"
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s

{% if openedx_forum_job %}
  forum-job:
    image: {{ openedx_forum_docker_image }}
    entrypoint: {{ openedx_forum_job_docker_entrypoint }}
    environment:
{% for env_var_name, env_var_value in openedx_forum_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    # From https://github.com/overhangio/tutor-forum/blob/d03cb74eecd8397a728be32e1737e3edb9e13a35/tutorforum/templates/forum/hooks/forum/init
    deploy:
      replicas: 1 # should exist only 1 instance of this container
      mode: replicated # after docker 22.06.0 release change it to replicated-job
      restart_policy:
        condition: on-failure # restart this container only if it fails to run
      placement:
{% if openedx_forum_job_deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_forum_job == true
{% endif %}
      resources:
        limits:
          cpus: "{{ openedx_forum_limit_cpus }}"
{% endif %}

  notes:
    image: {{ openedx_notes_docker_image }}
    environment:
{% for env_var_name, env_var_value in openedx_notes_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ openedx_notes_replicas }}
      update_config:
        parallelism: {{ openedx_notes_deploy_parallelism }}
      placement:
        max_replicas_per_node: {{ ( openedx_notes_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
{% if openedx_notes_deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_notes == true
{% endif %}
      resources:
        limits:
          memory: 1G
          cpus: "{{ openedx_notes_limit_cpus }}"
        reservations:
          memory: 150M
    healthcheck:
      test: |
        bash -c 'exec 3<>/dev/tcp/127.0.0.1/{{ openedx_notes_port }} && echo -e "GET /heartbeat/ HTTP/1.1\r\nHost: notes\r\nConnection: close\r\n\r\n" >&3 && cat <&3 | grep "HTTP/1.1 200 OK"'
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s
{{ helper.service_configs(service='notes') }}
{{ helper.service_secrets(service='notes') }}

{% if openedx_notes_job %}
  notes-job:
    image: {{ openedx_notes_docker_image }}
    environment:
{% for env_var_name, env_var_value in openedx_notes_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    command: {{ openedx_notes_job_command }}
    deploy:
      replicas: 1 # should exist only 1 instance of this container
      mode: replicated # after docker 22.06.0 release change it to replicated-job
      restart_policy:
        condition: on-failure # restart this container only if it fails to run
      placement:
{% if openedx_notes_job_deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_notes_job == true
{% endif %}
      resources:
        limits:
          cpus: "{{ openedx_notes_limit_cpus }}"
{{ helper.service_configs(service='notes') }}
{{ helper.service_secrets(service='notes') }}
{% endif %}

  discovery:
    image: {{ openedx_discovery_docker_image }}
    environment:
{% for env_var_name, env_var_value in openedx_discovery_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ openedx_discovery_replicas }}
      update_config:
        parallelism: {{ openedx_discovery_deploy_parallelism }}
      placement:
        max_replicas_per_node: {{ ( openedx_discovery_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
{% if openedx_discovery_deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_discovery == true
{% endif %}
      resources:
        limits:
          memory: 400M
          cpus: "{{ openedx_discovery_limit_cpus }}"
        reservations:
          memory: 210M
    healthcheck:
      # Check if /health/ returns a HTTP status 200, meaning the service is ok
      test: curl --silent --fail --head --resolve discovery:8000:127.0.0.1 http://discovery:8000/health/ | egrep "HTTP/.*200 OK"
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s
{{ helper.service_configs(service='discovery') }}
{{ helper.service_secrets(service='discovery') }}

{% if openedx_discovery_job %}
  discovery-job:
    image: {{ openedx_discovery_docker_image }}
    environment:
{% for env_var_name, env_var_value in openedx_discovery_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    command: {{ openedx_discovery_job_command }}
    deploy:
      replicas: 1 # should exist only 1 instance of this container
      mode: replicated # after docker 22.06.0 release change it to replicated-job
      restart_policy:
        condition: on-failure # restart this container only if it fails to run
      placement:
{% if openedx_discovery_job_deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_discovery_job == true
{% endif %}
      resources:
        limits:
          cpus: "{{ openedx_discovery_limit_cpus }}"
{{ helper.service_configs(service='discovery') }}
{{ helper.service_secrets(service='discovery') }}
{% endif %}

  smtp:
    image: {{ openedx_smtp_docker_image }}
    environment:
{% for env_var_name, env_var_value in openedx_smtp_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ openedx_smtp_replicas }}
      update_config:
        parallelism: {{ openedx_smtp_deploy_parallelism }}
      placement:
        max_replicas_per_node: {{ ( openedx_smtp_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
{% if openedx_smtp_deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_smtp == true
{% endif %}
      resources:
        limits:
          memory: 50M
          cpus: "{{ openedx_smtp_limit_cpus }}"
        reservations:
          memory: 10M
    healthcheck:
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/8025", "||", "exit", "1"]
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s

{% for redis_container_name, redis_config in openedx_redis_containers.items() %}
  {{ redis_container_name }}:
    image: {{ openedx_redis_docker_image }}
    environment:
      # Use custom timezone
      TZ: {{ openedx_timezone | default("Europe/Lisbon") }}
      ALLOW_EMPTY_PASSWORD: "yes"
{% if redis_config['mode'] is defined %}
      REDIS_MASTER_HOST: {{ openedx_docker_deploy_stack_name }}_{{ redis_container_name }}
      REDIS_REPLICATION_MODE: {{ redis_config.mode }}
{% endif %}
      # announce the hostname of the container, because the IP address can change after a container restart
      REDIS_REPLICA_IP: {{ openedx_docker_deploy_stack_name }}_{{ redis_container_name }}
    volumes:
      - {{ redis_config.dest }}:/bitnami/redis/data
    deploy:
      replicas: 1
      endpoint_mode: dnsrr
      resources:
        limits:
          memory: 1G
          cpus: "{{ openedx_limit_cpus }}"
        reservations:
          memory: 200M
      placement:
        max_replicas_per_node: 1
{% if redis_config.host_fqdn is defined and redis_config.host_fqdn is not none %}
        constraints:
          - node.labels.fqdn == {{ redis_config.host_fqdn }}
{% endif %}
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s

{% endfor %}

{% for elasticsearch_container_name, elasticsearch_container in openedx_elasticsearch_containers.items() %}

  {{ elasticsearch_container_name }}:
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.4"
    environment:
      # Use custom timezone
      TZ: {{ openedx_timezone | default("Europe/Lisbon") }}
      # The elastic search node name has the same name of the container
      node.name: {{ elasticsearch_container_name }}
      # Disable the memory swapping
      bootstrap.memory_lock: "true"
      # The name of the cluster. A node can only join a cluster when it shares its cluster.name with all the other nodes in the cluster.
      cluster.name: openedx-es-cluster
      # By default only binds to loopback addresses, this binds to any network
      network.host: "0.0.0.0"
      #discovery.seed_hosts: {{ openedx_elasticsearch_containers.keys() | list | difference([elasticsearch_container_name]) | list | join(',') }}
      discovery.seed_hosts: {{ openedx_elasticsearch_containers.keys() | list | join(',') }}
{% if ( openedx_elasticsearch_initialization | default(false) ) %}
      # Only define on first cluster configuration
      cluster.initial_master_nodes: {{ openedx_elasticsearch_containers.keys() | first }}
{% endif %}
      ES_JAVA_OPTS: -Xms1g -Xmx1g -Des.enforce.bootstrap.checks=true
         # -Des.index.number_of_replicas=2
      xpack.security.enabled: "false"
      xpack.security.http.ssl.enabled: "false"
      xpack.security.transport.ssl.enabled: "false"
      xpack.ml.enabled: "false"
      xpack.graph.enabled: "false"
      xpack.watcher.enabled: "false"
      xpack.monitoring.enabled: "false"
      xpack.monitoring.collection.enabled: "false"
    user: "1000:1000"
    volumes:
      - {{ elasticsearch_container.data }}:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
    deploy:
      replicas: 1
      endpoint_mode: dnsrr
      resources:
        limits:
          memory: 3G
          cpus: "{{ openedx_limit_cpus }}"
        reservations:
          memory: 1.5G
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.fqdn == {{ elasticsearch_container.host_fqdn }}
    healthcheck:
      # We can't run an healthcheck against the 9200 HTTP port, because it requires that this
      # node should be correctly connected to the cluster, but the other nodes only resolve this
      # node DNS when this container is on healthy state.
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/9300", "||", "exit", "1"]
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s
{% endfor %}

  # HAProxy services
  # Because of a docker swarm limitation, we need to have 2 similar haproxy services.
  # Both have an exactly equal configuration, `haproxy.cfg` file.
  # The difference are the published ports and the `endpoint_mode`.
  # The inside has an `endpoint_mode` of `dns_rr`, so clients can have a longer connection
  # and don't receive a connection timeout.
  # The outside has the default `vip` value on `endpoint_mode` and published ports using
  haproxy:
    image: {{ openedx_haproxy_docker_image }}
    ports:
      # HaProxy statistics http
      - target:    {{ openedx_haproxy_statistics_port }}
        published: {{ openedx_haproxy_statistics_port }}
        protocol:  tcp
        mode:      ingress
      # MySQL write port, that returns connections to primary MySQL node,
      # but falls back to the secondary cluster container
      - target:    {{ openedx_haproxy_write_port }}
        published: {{ openedx_haproxy_write_port }}
        protocol:  tcp
        mode:      ingress
      # MySQL read port, that returns connections to secondary MySQL node,
      # but falls back to the primary cluster container
      - target:    {{ openedx_haproxy_readonly_port }}
        published: {{ openedx_haproxy_readonly_port }}
        protocol:  tcp
        mode:      ingress
      # Balanced elasticsearch port, used to monitoring, it resolves to one of the elasticsearch containers
      - target:    {{ openedx_haproxy_elasticsearch_port }}
        published: {{ openedx_haproxy_elasticsearch_port }}
        protocol:  tcp
      # Redis primary
      - target:    {{ openedx_haproxy_redis_port }}
        published: {{ openedx_haproxy_redis_port }}
        protocol:  tcp
{% for mysql_container_name, mysql_container in openedx_mysql_containers.items() %}
      # Allow connections to {{ mysql_container_name }}
      - target:    {{ mysql_container.outside_port }}
        published: {{ mysql_container.outside_port }}
        protocol:  tcp
        mode:      ingress
{% endfor %}
    environment:
      # Use custom timezone
      TZ: {{ openedx_timezone | default("Europe/Lisbon") }}
    deploy:
      replicas: {{ openedx_haproxy_replicas }}
      endpoint_mode: vip # the haproxy outside we use the default vip
      placement:
        max_replicas_per_node: {{ ( openedx_haproxy_replicas / docker_swarm_cluster_group_length | int ) | round(0, 'ceil') | int }}
{% if openedx_haproxy_deploy_constraint | default(false) | bool %}
        constraints:
          - node.labels.openedx_haproxy == true
{% endif %}
      resources:
        limits:
          memory: 300M
          cpus: "{{ openedx_limit_cpus }}"
        reservations:
          memory: 200M
      update_config:
        # We are updating 1 container a time
        parallelism: 1
    healthcheck:
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/{{ openedx_haproxy_statistics_port }}", "||", "exit", "1"]
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s
{{ helper.service_configs(service='haproxy') }}
{{ helper.service_secrets(service='haproxy') }}

{% for mysql_container_name, mysql_container in openedx_mysql_containers.items() %}

  {{ mysql_container_name }}:
    image: {{ openedx_mysql_docker_image | default("docker.io/mysql:5.7.37-debian") }}
    hostname: {{ mysql_container.hostname }}
    command: mysqld --character-set-server=utf8 --collation-server=utf8_general_ci --innodb-buffer-pool-size=4G --innodb-log-file-size=1G --innodb-log-buffer-size=256M{{ (' ' + openedx_mysql_replica_extra_parameters) if not mysql_container.primary else '' }}
      # Older DB had: --slave-skip-errors=1032,1062,1451,1452
    environment:
      # Use custom timezone
      TZ: {{ openedx_timezone | default("Europe/Lisbon") }}
      MYSQL_DATABASE: "{{ EDXAPP_MYSQL_DB_NAME }}"
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/mysql-root-password
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping --silent -p{{ EDXAPP_MYSQL_PASSWORD_ADMIN }} || exit 1"]
      start_period: 30s
      retries: 10
      interval: 30s
      timeout: 30s
    volumes:
      - {{ mysql_container.data }}:/var/lib/mysql
    deploy:
      replicas: 1
      endpoint_mode: dnsrr
      resources:
        limits:
          memory: 8G
{% if mysql_container.limit_cpus %}
          cpus: "{{ mysql_container.limit_cpus }}"
{% endif %}
        reservations:
          memory: 5G
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.fqdn == {{ mysql_container.host_fqdn }}
{{ helper.service_configs(service=mysql_container_name) }}
{{ helper.service_secrets(service=mysql_container_name) }}
{% endfor %}

{{ helper.configs() }}
{{ helper.secrets() }}
