{% import '_docker_deploy_helper.j2' as helper with context %}
# {{ ansible_managed }}
version: "3.8"

services:
{% for elasticsearch_container_name, elasticsearch_container in openedx_elasticsearch_containers.items() %}

  {{ elasticsearch_container_name }}:
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.4"
    environment:
      # The elastic search node name has the same name of the container
      node.name: {{ elasticsearch_container_name }}
      # Disable the memory swapping
      bootstrap.memory_lock: "true"
      # The name of the cluster. A node can only join a cluster when it shares its cluster.name with all the other nodes in the cluster.
      cluster.name: openedx-es-cluster
      # By default only binds to loopback addresses, this binds to any network
      network.host: "0.0.0.0"
      #discovery.seed_hosts: {{ openedx_elasticsearch_containers.keys() | list | difference([elasticsearch_container_name]) | list | join(',') }}
      discovery.seed_hosts: {{ openedx_elasticsearch_containers.keys() | list | join(',') }}
{% if ( openedx_elasticsearch_cluster_initialization | default(false) ) %}
      # Only define on first cluster configuration
      cluster.initial_master_nodes: {{ openedx_elasticsearch_containers.keys() | first }}
{% endif %}
      ES_JAVA_OPTS: -Xms1g -Xmx1g -Des.enforce.bootstrap.checks=true
         # -Des.index.number_of_replicas=2
      xpack.security.enabled: "false"
      xpack.security.http.ssl.enabled: "false"
      xpack.security.transport.ssl.enabled: "false"
      xpack.ml.enabled: "false"
      xpack.graph.enabled: "false"
      xpack.watcher.enabled: "false"
      xpack.monitoring.enabled: "false"
      xpack.monitoring.collection.enabled: "false"
    user: "1000:1000"
    volumes:
      - {{ elasticsearch_container.data }}:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
    deploy:
      replicas: 1
      mode: replicated
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 1.5G
        reservations:
          memory: 500M
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.fqdn == {{ elasticsearch_container.host_fqdn }}
    healthcheck:
      # We can't run an healthcheck against the 9200 HTTP port, because it requires that this
      # node should be correctly connected to the cluster, but the other nodes only resolve this
      # node DNS when this container is on healthy state.
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/9300", "||", "exit", "1"]
      retries: 30
      interval: 10s
      timeout: 10s
{% endfor %}

  load_balancer_haproxy:
    image: {{ openedx_load_balancer_haproxy_docker_image | default('haproxy:2.4.16') }}
    ports:
      # write
      - target:    {{ openedx_load_balancer_haproxy_write_port | default(3306) }}
        published: {{ openedx_load_balancer_haproxy_write_port | default(3306) }}
        protocol:  tcp
        mode:      ingress
      # read
      - target:    {{ openedx_load_balancer_haproxy_readonly_port | default(3307) }}
        published: {{ openedx_load_balancer_haproxy_readonly_port | default(3307) }}
        protocol:  tcp
        mode:      ingress
      # statistics
      - target:    {{ openedx_load_balancer_haproxy_statistics_port | default(1936) }}
        published: {{ openedx_load_balancer_haproxy_statistics_port | default(1936) }}
        protocol:  tcp
        # each container have a statistics page but we should view only one on each host
        mode:      host
      # Allow to connect to elasticsearch cluster outside the docker stack like so we can easily
      # monitor the cluster from the Makefile
      - target:    9200
        published: {{ openedx_load_balancer_haproxy_elasticsearch_port }}
        protocol: tcp
{{ helper.service_configs(service='load_balancer_haproxy') }}
{{ helper.service_secrets(service='load_balancer_haproxy') }}
    deploy:
      replicas: {{ openedx_load_balancer_haproxy_deploy_replicas_count | default(2) }}
      placement:
        max_replicas_per_node: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 300M
      # we can't start-first because we have statistics port using host mode
      # nevertheless because still can have this because we have at least 2 replicas 
      # and we are deploy only one each time
      update_config:
        parallelism: 1
        failure_action: rollback
        order: stop-first
    healthcheck:
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/{{ openedx_load_balancer_haproxy_write_port | default(3306) }}", "&&", "echo", "''", ">", "/dev/tcp/127.0.0.1/{{ openedx_load_balancer_haproxy_readonly_port | default(3306) }}", "||", "exit", "1"]
      start_period: 3s
      retries: 10
      interval: 10s
      timeout: 10s

{% for mysql_container_name, mysql_container in openedx_mysql_containers.items() %}

  {{ mysql_container_name }}:
    image: {{ openedx_mysql_docker_image | default("docker.io/mysql:5.7.37-debian") }}
    hostname: {{ mysql_container.hostname }}
    environment:
      MYSQL_DATABASE: "{{ EDXAPP_MYSQL_DB_NAME }}"
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/mysql-root-password
{{ helper.service_configs(service=mysql_container_name) }}
{{ helper.service_secrets(service=mysql_container_name) }}
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping --silent -p{{ EDXAPP_MYSQL_PASSWORD_ADMIN }} || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 30
      start_period: 10s
    volumes:
      - {{ mysql_container.data }}:/var/lib/mysql
    ports:
      # In future after the migration of everything to swarm, limit this connection to localhost
      # example with: "127.0.0.1:3306:3306"
      - target: 3306
        published: {{ mysql_container.ingress_port }}
        protocol: tcp
        mode: ingress
    deploy:
      replicas: 1
      mode: replicated
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 4G
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.fqdn == {{ mysql_container.host_fqdn }}
{% endfor %}

{{ helper.configs() }}
{{ helper.secrets() }}
