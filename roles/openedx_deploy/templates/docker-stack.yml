{% import '_docker_deploy_helper.j2' as helper with context %}
# {{ ansible_managed }}
version: "3.8"

services:

  # TODO: add "mongo" cluster

  nginx:
    image: {{ openedx_nginx_image }}
    ports:
      - target:    {{ openedx_nginx_http }}
        published: {{ openedx_nginx_http_ingress_port }}
        protocol:  tcp
        mode:      ingress
      - target:    {{ openedx_nginx_https }}
        published: {{ openedx_nginx_https_ingress_port }}
        protocol:  tcp
        mode:      ingress
    deploy:
      replicas: {{ openedx_nginx_replicas }}
      update_config:
        # use the default 'stop-first' so we can detect on replicas health check that something is still not converged.
        order: stop-first
        # rolling deploy with half the replicas, if replicas has an odd number, when round to a lower number,
        # so in the limit we have 3 update batches, 2 bigger half batches and +1 at the end.
        # Example:
        #    #replicas -> #parallelism
        #    2 --> 1
        #    3 --> 1
        #    4 --> 2
        #    5 --> 2
        parallelism: {{ [ 1, ( openedx_nginx_replicas / 2 | int ) ] | max | round(0, 'floor') | int }}
      placement:
        max_replicas_per_node: {{ ( openedx_nginx_replicas / docker_swarm_cluster_group_length | int ) | round(0, 'ceil') | int }}
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 5M
    extra_hosts:
      - "{{ nau_ceph_host }}:{{ nau_ceph_s3_ip }}"
    healthcheck:
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/80", "&&", "echo", "''", ">", "/dev/tcp/127.0.0.1/443", "||", "exit", "1"]
      start_period: 3s
      retries: 10
      interval: 10s
      timeout: 10s
{{ helper.service_configs(service='nginx') }}
{{ helper.service_secrets(service='nginx') }}

  lms:
    image: {{ openedx_app_image }}
    environment:
{% for env_var_name, env_var_value in openedx_lms_env_var.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ openedx_lms_replicas }}
      placement:
        max_replicas_per_node: {{ ( openedx_lms_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
    healthcheck:
      # Check if heartbeat is ok
      test: curl --silent --fail --head --resolve lms:8000:127.0.0.1 http://lms:8000/heartbeat | egrep "HTTP/.*200 OK"
      retries: 30
      interval: 10s
      timeout: 10s
{{ helper.service_configs(service='edxapp') }}
{{ helper.service_secrets(service='edxapp') }}

{% if openedx_app_job %}
  app-job:
    image: {{ openedx_app_image }}
    environment:
{% for env_var_name, env_var_value in openedx_app_env_var.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    command: {{ openedx_app_job_command }}
    deploy:
      replicas: 1 # should exist only 1 instance of this container
      mode: replicated # after docker 22.06.0 release change it to replicated-job
      restart_policy:
        condition: on-failure # restart this container only if it fails to run
{{ helper.service_configs(service='edxapp') }}
{{ helper.service_secrets(service='edxapp') }}
{% endif %}

  cms:
    image: {{ openedx_app_image }}
    environment:
{% for env_var_name, env_var_value in openedx_cms_env_var.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ openedx_cms_replicas }}
      placement:
        max_replicas_per_node: {{ ( openedx_cms_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
    healthcheck:
      # Check if heartbeat is ok
      test: curl --silent --fail --head --resolve cms:8000:127.0.0.1 http://cms:8000/heartbeat | egrep "HTTP/.*200 OK"
      retries: 30
      interval: 10s
      timeout: 10s
{{ helper.service_configs(service='edxapp') }}
{{ helper.service_secrets(service='edxapp') }}

  lms-worker:
    image: {{ openedx_app_image }}
    environment:
{% for env_var_name, env_var_value in openedx_lms_worker_env_var.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    command: celery --app=lms.celery worker --loglevel=info --hostname=edx.lms.core.default.%%h --max-tasks-per-child=100 --exclude-queues=edx.cms.core.default
    deploy:
      replicas: {{ openedx_lms_worker_replicas }}
      placement:
        max_replicas_per_node: {{ ( openedx_lms_worker_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
{{ helper.service_configs(service='edxapp') }}
{{ helper.service_secrets(service='edxapp') }}

  cms-worker:
    image: {{ openedx_app_image }}
    environment:
{% for env_var_name, env_var_value in openedx_cms_worker_env_var.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    command: celery --app=cms.celery worker --loglevel=info --hostname=edx.cms.core.default.%%h --max-tasks-per-child 100 --exclude-queues=edx.lms.core.default
    deploy:
      replicas: {{ openedx_cms_worker_replicas }}
      placement:
        max_replicas_per_node: {{ ( openedx_cms_worker_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
{{ helper.service_configs(service='edxapp') }}
{{ helper.service_secrets(service='edxapp') }}

  forum:
    image: {{ openedx_forum_docker_image }}
    entrypoint: {{ openedx_forum_docker_entrypoint }}
    environment:
{% for env_var_name, env_var_value in openedx_forum_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    healthcheck:
      # Check if forum heartbeat returns ok
      test: wget -qSO- 127.0.0.1:{{ openedx_forum_port }}/heartbeat 2>&1 | egrep "HTTP/.*200 OK"
      retries: 30
      interval: 10s
      timeout: 10s
    deploy:
      replicas: {{ openedx_forum_replicas }}
      placement:
        max_replicas_per_node: {{ ( openedx_forum_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}

{% if openedx_forum_job %}
  forum-job:
    image: {{ openedx_forum_docker_image }}
    entrypoint: {{ openedx_forum_job_docker_entrypoint }}
    environment:
{% for env_var_name, env_var_value in openedx_forum_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    # From https://github.com/overhangio/tutor-forum/blob/d03cb74eecd8397a728be32e1737e3edb9e13a35/tutorforum/templates/forum/hooks/forum/init
    deploy:
      replicas: 1 # should exist only 1 instance of this container
      mode: replicated # after docker 22.06.0 release change it to replicated-job
      restart_policy:
        condition: on-failure # restart this container only if it fails to run
{% endif %}

  notes:
    image: {{ openedx_notes_docker_image }}
    environment:
{% for env_var_name, env_var_value in openedx_notes_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ openedx_notes_replicas }}
      placement:
        max_replicas_per_node: {{ ( openedx_notes_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
    healthcheck:
      # Check if forum heartbeat returns ok
      test: |
        bash -c 'exec 3<>/dev/tcp/127.0.0.1/{{ openedx_notes_port }} && echo -e "GET /heartbeat/ HTTP/1.1\r\nHost: notes\r\nConnection: close\r\n\r\n" >&3 && cat <&3 | grep "HTTP/1.1 200 OK"'
      retries: 30
      interval: 10s
      timeout: 10s
{{ helper.service_configs(service='notes') }}
{{ helper.service_secrets(service='notes') }}

{% if openedx_notes_job %}
  notes-job:
    image: {{ openedx_notes_docker_image }}
    environment:
{% for env_var_name, env_var_value in openedx_notes_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    command: {{ openedx_notes_job_command }}
    deploy:
      replicas: 1 # should exist only 1 instance of this container
      mode: replicated # after docker 22.06.0 release change it to replicated-job
      restart_policy:
        condition: on-failure # restart this container only if it fails to run
{{ helper.service_configs(service='notes') }}
{{ helper.service_secrets(service='notes') }}
{% endif %}

  discovery:
    image: {{ openedx_discovery_docker_image }}
    environment:
{% for env_var_name, env_var_value in openedx_discovery_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ openedx_discovery_replicas }}
      placement:
        max_replicas_per_node: {{ ( openedx_discovery_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}
{{ helper.service_configs(service='discovery') }}
{{ helper.service_secrets(service='discovery') }}

{% if openedx_discovery_job %}
  discovery-job:
    image: {{ openedx_discovery_docker_image }}
    environment:
{% for env_var_name, env_var_value in openedx_discovery_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    command: {{ openedx_discovery_job_command }}
    deploy:
      replicas: 1 # should exist only 1 instance of this container
      mode: replicated # after docker 22.06.0 release change it to replicated-job
      restart_policy:
        condition: on-failure # restart this container only if it fails to run
{{ helper.service_configs(service='discovery') }}
{{ helper.service_secrets(service='discovery') }}
{% endif %}

  smtp:
    image: {{ openedx_smtp_docker_image }}
    environment:
{% for env_var_name, env_var_value in openedx_smtp_docker_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ openedx_smtp_replicas }}
      placement:
        max_replicas_per_node: {{ ( openedx_smtp_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}

  redis-sentinel:
    image: {{ openedx_redis_sentinel_docker_image }}
    environment:
      # Use custom timezone
      TZ: {{ openedx_timezone | default("Europe/Lisbon") }}
      REDIS_MASTER_HOST: {{ openedx_docker_deploy_stack_name }}_{{ openedx_redis_primary_container }}
      # Accept host names
      REDIS_SENTINEL_RESOLVE_HOSTNAMES: "yes"
      # Enabling the announce-hostnames global configuration makes Sentinel use host names instead. 
      # This affects replies to clients, values written in configuration files, 
      # the REPLICAOF command issued to replicas, etc.
      REDIS_SENTINEL_ANNOUNCE_HOSTNAMES: "yes"
      # announce the hostname of the container
      # REDIS_SENTINEL_ANNOUNCE_IP:
    deploy:
      replicas: {{ openedx_redis_sentinel_replicas }}
      # Use DNS Round Robin to bypass the docker routing mesh and to allow HAProxy parse the DNS
      # entries to discover all the redis sentinel instance replicas.
      endpoint_mode: dnsrr
      placement:
        max_replicas_per_node: {{ ( openedx_redis_sentinel_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}

{% for redis_container_name, redis_config in openedx_redis_containers.items() %}
  {{ redis_container_name }}:
    image: {{ openedx_redis_docker_image }}
    environment:
      # Use custom timezone
      TZ: {{ openedx_timezone | default("Europe/Lisbon") }}
      ALLOW_EMPTY_PASSWORD: "yes"
      REDIS_MASTER_HOST: {{ openedx_docker_deploy_stack_name }}_{{ openedx_redis_primary_container }}
      REDIS_REPLICATION_MODE: {{ redis_config.mode }}
      # announce the hostname of the container, because the IP address can change after a container restart
      REDIS_REPLICA_IP: {{ openedx_docker_deploy_stack_name }}_{{ redis_container_name }}
    volumes:
      - {{ redis_config.data }}:/bitnami/redis/data
    deploy:
      replicas: 1
      resources:
        limits:
          memory: 4G
      placement:
        max_replicas_per_node: 1
{% if redis_config.host_fqdn is defined and redis_config.host_fqdn is not none %}
        constraints:
          - node.labels.fqdn == {{ redis_config.host_fqdn }}
{% endif %}

{% endfor %}

{% for elasticsearch_container_name, elasticsearch_container in openedx_elasticsearch_containers.items() %}

  {{ elasticsearch_container_name }}:
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.4"
    environment:
      # Use custom timezone
      TZ: {{ openedx_timezone | default("Europe/Lisbon") }}
      # The elastic search node name has the same name of the container
      node.name: {{ elasticsearch_container_name }}
      # Disable the memory swapping
      bootstrap.memory_lock: "true"
      # The name of the cluster. A node can only join a cluster when it shares its cluster.name with all the other nodes in the cluster.
      cluster.name: openedx-es-cluster
      # By default only binds to loopback addresses, this binds to any network
      network.host: "0.0.0.0"
      #discovery.seed_hosts: {{ openedx_elasticsearch_containers.keys() | list | difference([elasticsearch_container_name]) | list | join(',') }}
      discovery.seed_hosts: {{ openedx_elasticsearch_containers.keys() | list | join(',') }}
{% if ( openedx_elasticsearch_initialization | default(false) ) %}
      # Only define on first cluster configuration
      cluster.initial_master_nodes: {{ openedx_elasticsearch_containers.keys() | first }}
{% endif %}
      ES_JAVA_OPTS: -Xms1g -Xmx1g -Des.enforce.bootstrap.checks=true
         # -Des.index.number_of_replicas=2
      xpack.security.enabled: "false"
      xpack.security.http.ssl.enabled: "false"
      xpack.security.transport.ssl.enabled: "false"
      xpack.ml.enabled: "false"
      xpack.graph.enabled: "false"
      xpack.watcher.enabled: "false"
      xpack.monitoring.enabled: "false"
      xpack.monitoring.collection.enabled: "false"
    user: "1000:1000"
    volumes:
      - {{ elasticsearch_container.data }}:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
    deploy:
      replicas: 1
      endpoint_mode: dnsrr
      resources:
        limits:
          memory: 1.5G
        reservations:
          memory: 500M
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.fqdn == {{ elasticsearch_container.host_fqdn }}
    healthcheck:
      # We can't run an healthcheck against the 9200 HTTP port, because it requires that this
      # node should be correctly connected to the cluster, but the other nodes only resolve this
      # node DNS when this container is on healthy state.
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/9300", "||", "exit", "1"]
      retries: 30
      interval: 10s
      timeout: 10s
{% endfor %}

  load_balancer_haproxy:
    image: {{ openedx_load_balancer_haproxy_docker_image | default('haproxy:2.4.16') }}
    ports:
      # MySQL write port, that returns connections to primary MySQL node,
      # but falls back to the secondary cluster container
      - target:    {{ openedx_load_balancer_haproxy_write_port | default(3306) }}
        published: {{ openedx_load_balancer_haproxy_write_port | default(3306) }}
        protocol:  tcp
        mode:      ingress
      # MySQL read port, that returns connections to secondary MySQL node,
      # but falls back to the primary cluster container
      - target:    {{ openedx_load_balancer_haproxy_readonly_port | default(3307) }}
        published: {{ openedx_load_balancer_haproxy_readonly_port | default(3307) }}
        protocol:  tcp
        mode:      ingress
      # HaProxy statistics http
      - target:    {{ openedx_load_balancer_haproxy_statistics_port | default(1936) }}
        published: {{ openedx_load_balancer_haproxy_statistics_port | default(1936) }}
        protocol:  tcp
        mode:      ingress
      # Balanced elasticsearch
      - target:    9200
        published: {{ openedx_load_balancer_haproxy_elasticsearch_port }}
        protocol:  tcp
      # Redis DB primary node
      - target:    6379
        published: {{ openedx_load_balancer_haproxy_redis_port }}
        protocol:  tcp
{% for mysql_container_name, mysql_container in openedx_mysql_containers.items() %}
      # Allow connections to {{ mysql_container_name }} even it uses `dnsrr`.
      - target:    {{ mysql_container.outside_port }}
        published: {{ mysql_container.outside_port }}
        protocol:  tcp
        mode:      ingress
{% endfor %}
{{ helper.service_configs(service='load_balancer_haproxy') }}
{{ helper.service_secrets(service='load_balancer_haproxy') }}
    environment:
      # Use custom timezone
      TZ: {{ openedx_timezone | default("Europe/Lisbon") }}
    deploy:
      replicas: {{ openedx_load_balancer_haproxy_deploy_replicas_count | default(2) }}
      placement:
        max_replicas_per_node: 1
      resources:
        limits:
          memory: 300M
      # we can't start-first because we have statistics port using host mode
      # nevertheless because still can have this because we have at least 2 replicas 
      # and we are deploy only one each time
      update_config:
        parallelism: 1
        failure_action: rollback
        order: stop-first
    healthcheck:
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/{{ openedx_load_balancer_haproxy_write_port | default(3306) }}", "&&", "echo", "''", ">", "/dev/tcp/127.0.0.1/{{ openedx_load_balancer_haproxy_readonly_port | default(3306) }}", "||", "exit", "1"]
      start_period: 3s
      retries: 10
      interval: 10s
      timeout: 10s

{% for mysql_container_name, mysql_container in openedx_mysql_containers.items() %}

  {{ mysql_container_name }}:
    image: {{ openedx_mysql_docker_image | default("docker.io/mysql:5.7.37-debian") }}
    hostname: {{ mysql_container.hostname }}
    command: mysqld --character-set-server=utf8 --collation-server=utf8_general_ci
    environment:
      # Use custom timezone
      TZ: {{ openedx_timezone | default("Europe/Lisbon") }}
      MYSQL_DATABASE: "{{ EDXAPP_MYSQL_DB_NAME }}"
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/mysql-root-password
{{ helper.service_configs(service=mysql_container_name) }}
{{ helper.service_secrets(service=mysql_container_name) }}
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping --silent -p{{ EDXAPP_MYSQL_PASSWORD_ADMIN }} || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 30
      start_period: 10s
    volumes:
      - {{ mysql_container.data }}:/var/lib/mysql
    deploy:
      replicas: 1
      endpoint_mode: dnsrr
      resources:
        limits:
          memory: 4G
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.fqdn == {{ mysql_container.host_fqdn }}
{% endfor %}

{{ helper.configs() }}
{{ helper.secrets() }}
