{% import '_docker_deploy_helper.j2' as helper with context %}
version: "3.8"

services:
{% for richie_site, richie_site_config in richie_sites.items() %}
  {{ richie_site }}_app:
    # We tag our images with the current commit sha1 in the CI to make them
    # unique and avoid collisions in parallel builds.
    image: {{ richie_site_config.app_image }}
    environment:
{% for env_var_name, env_var_value in richie_site_config.app_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      replicas: {{ richie_site_config.app_replicas }}
      mode: replicated
      restart_policy:
        condition: on-failure
      update_config:
        # start 1 container a time, so migrations run only on the new container
        parallelism: 1
        failure_action: rollback
        order: start-first
      resources:
        limits:
          memory: 2.5G
        reservations:
          memory: 500M
      placement:
        max_replicas_per_node: 1
    # volumes:
      # Used to increase log verbosity of the gunicorn and to add a Gunicorn worker abort handler
      # - ./app.py:/usr/local/etc/gunicorn/app.py
    extra_hosts:
      - "{{ nau_ceph_host }}:{{ nau_ceph_s3_ip }}"
    depends_on:
      - db
      - elasticsearch
      #- redis-sentinel
      - redis
    user: "{{ richie_site_config.app_docker_user | default('1000') }}"
    command: wait-for-it redis-sentinel:26379 -- wait-for-it load_balancer_haproxy:3306 -- /bin/bash -c "python manage.py migrate && gunicorn -c /usr/local/etc/gunicorn/app.py nau.wsgi:application"
    healthcheck:
      test: curl --silent --fail --resolve {{ richie_site_config.app_environment_variables.DJANGO_ALLOWED_HOSTS }}:8000:127.0.0.1 'http://{{ richie_site_config.app_environment_variables.DJANGO_ALLOWED_HOSTS }}:8000'
      interval: 30s
      timeout: 10s
      retries: 30
      start_period: 10s
{{ helper.service_configs(service=richie_site+'_app') }}
{{ helper.service_secrets(service=richie_site+'_app') }}

  # Job container that migrates db and reindex
  {{ richie_site }}_app_job:
    # We tag our images with the current commit sha1 in the CI to make them
    # unique and avoid collisions in parallel builds.
    image: {{ richie_site_config.app_image }}
    # use an environment variables file so we can reuse them bellow
    environment:
{% for env_var_name, env_var_value in richie_site_config.app_environment_variables.items() %}
      {{ env_var_name }}: "{{ env_var_value }}"
{% endfor %}
    deploy:
      # should exist only 1 instance of this container
      replicas: 1
      mode: replicated # after docker 22.06.0 release change it to replicated-job
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 2.5G
        reservations:
          memory: 500M
      placement:
        max_replicas_per_node: 1
    extra_hosts:
      - "{{ nau_ceph_host }}:{{ nau_ceph_s3_ip }}"
    user: "{{ richie_site_config.app_docker_user | default('1000') }}"
    # wait for elasticsearch and mysql proxied on load balancer HAProxy
    command: wait-for-it load_balancer_haproxy:9200 -- wait-for-it load_balancer_haproxy:3306 -- /bin/bash -c "python manage.py migrate && sleep 1m && python manage.py bootstrap_elasticsearch"
    # apt update && apt install -y vim
    # vim /usr/local/lib/python3.10/site-packages/richie/apps/search/__init__.py
    # add line: 
    # **getattr(settings, "RICHIE_ES_CLIENT_KWARGS", { 'timeout': 120 }),
    # python manage.py bootstrap_elasticsearch
{{ helper.service_configs(service=richie_site+'_app') }}
{{ helper.service_secrets(service=richie_site+'_app') }}

  {{ richie_site }}_nginx:
    image: {{ richie_site_config.nginx_image }}
    ports:
      - target: 80
        published: {{ richie_site_config.nginx_http_ingress_port }}
        protocol: tcp
        mode: ingress
      - target: 443
        published: {{ richie_site_config.nginx_https_ingress_port }}
        protocol: tcp
        mode: ingress
    deploy:
      replicas: {{ richie_site_config.nginx_replicas }}
      mode: replicated
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 128M
        reservations:
          memory: 5M
      placement:
        max_replicas_per_node: 1
    extra_hosts:
      - "{{ nau_ceph_host }}:{{ nau_ceph_s3_ip }}"
    healthcheck:
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/80", "&&", "echo", "''", ">", "/dev/tcp/127.0.0.1/443", "||", "exit", "1"]
      start_period: 3s
      retries: 10
      interval: 10s
      timeout: 10s
{{ helper.service_configs(service=richie_site+'_nginx') }}
{{ helper.service_secrets(service=richie_site+'_nginx') }}
{% endfor %}

  load_balancer_haproxy:
    image: {{ richie_load_balancer_haproxy_docker_image | default('haproxy:2.4.16') }}
    ports:
      # statistics
      - target:    {{ richie_load_balancer_haproxy_statistics_port }}
        published: {{ richie_load_balancer_haproxy_statistics_port }}
        protocol:  tcp
        # each container have a statistics page but we should view only one on each host
        mode:      host
      # Allow to connect to elasticsearch cluster outside the docker stack like so we can easily
      # monitor the cluster from the Makefile
      - target:    9200
        published: {{ richie_load_balancer_haproxy_elasticsearch_port }}
        protocol: tcp
      - target:    6379
        published: {{ richie_load_balancer_haproxy_redis_port }}
        protocol: tcp
{{ helper.service_configs(service='load_balancer_haproxy') }}
{{ helper.service_secrets(service='load_balancer_haproxy') }}
    deploy:
      replicas: {{ richie_load_balancer_haproxy_deploy_replicas_count | default(2) }}
      placement:
        max_replicas_per_node: 1
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 300M
      update_config:
        # We can't start-first because we have statistics port using host mode
        order: stop-first
        # Nevertheless we have a multi node docker swarm cluster and we are updating 1 container a time
        parallelism: 1
        failure_action: rollback
    healthcheck:
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/{{ richie_load_balancer_haproxy_statistics_port }}", "||", "exit", "1"]
      start_period: 3s
      retries: 10
      interval: 10s
      timeout: 10s

{% for mysql_container_name, mysql_container in richie_mysql_containers.items() %}

  {{ mysql_container_name }}:
    image: {{ richie_mysql_docker_image | default("docker.io/mysql:8.0.29") }}
{% if mysql_container.hostname is defined %}
    hostname: {{ mysql_container.hostname }}
{% endif %}
    environment:
      MYSQL_ROOT_PASSWORD_FILE: /run/secrets/mysql-root-password
    # use legacy authentication plugin to fix: 
      # django.db.utils.OperationalError: (2059, "Authentication plugin 'caching_sha2_password' 
      # cannot be loaded: /usr/lib/x86_64-linux-gnu/mariadb18/plugin/caching_sha2_password.so: 
      # cannot open shared object file: No such file or directory
    command: --default-authentication-plugin=mysql_native_password
    healthcheck:
      test: ["CMD-SHELL", "mysqladmin ping --silent -p{{ EDXAPP_MYSQL_PASSWORD_ADMIN }} || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 30
      start_period: 10s
    volumes:
      - {{ mysql_container.data }}:/var/lib/mysql
    ports:
      # In future after the migration of everything to swarm, limit this connection to localhost
      # example with: "127.0.0.1:3306:3306"
      - target: 3306
        published: {{ mysql_container.ingress_port }}
        protocol: tcp
        mode: ingress
    deploy:
      replicas: 1
      mode: replicated
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 4G
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.fqdn == {{ mysql_container.host_fqdn }}
{{ helper.service_configs(service=mysql_container_name) }}
{{ helper.service_secrets(service=mysql_container_name) }}
{% endfor %}

{% for elasticsearch_container_name, elasticsearch_container in richie_elasticsearch_containers.items() %}

  {{ elasticsearch_container_name }}:
    image: "docker.elastic.co/elasticsearch/elasticsearch:7.17.4"
    environment:
      # The elastic search node name has the same name of the container
      node.name: {{ elasticsearch_container_name }}
      # Disable the memory swapping
      bootstrap.memory_lock: "true"
      # The name of the cluster. A node can only join a cluster when it shares its cluster.name with all the other nodes in the cluster.
      cluster.name: richie-es-cluster
      # By default only binds to loopback addresses, this binds to any network
      network.host: "0.0.0.0"
      #discovery.seed_hosts: {{ richie_elasticsearch_containers.keys() | list | difference([elasticsearch_container_name]) | list | join(',') }}
      discovery.seed_hosts: {{ richie_elasticsearch_containers.keys() | list | join(',') }}
{% if ( richie_elasticsearch_cluster_initialization | default(false) ) %}
      # Only define on first cluster configuration
      cluster.initial_master_nodes: {{ richie_elasticsearch_containers.keys() | first }}
{% endif %}
      ES_JAVA_OPTS: -Xms1g -Xmx1g -Des.enforce.bootstrap.checks=true
         # -Des.index.number_of_replicas=2
      xpack.security.enabled: "false"
      xpack.security.http.ssl.enabled: "false"
      xpack.security.transport.ssl.enabled: "false"
      xpack.ml.enabled: "false"
      xpack.graph.enabled: "false"
      xpack.watcher.enabled: "false"
      xpack.monitoring.enabled: "false"
      xpack.monitoring.collection.enabled: "false"
    user: "1000:1000"
    volumes:
      - {{ elasticsearch_container.data }}:/usr/share/elasticsearch/data
    ulimits:
      memlock:
        soft: -1
        hard: -1
    deploy:
      replicas: 1
      mode: replicated
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 1.5G
        reservations:
          memory: 500M
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.fqdn == {{ elasticsearch_container.host_fqdn }}
    healthcheck:
      # We can't run an healthcheck against the 9200 HTTP port, because it requires that this
      # node should be correctly connected to the cluster, but the other nodes only resolve this
      # node DNS when this container is on healthy state.
      test: ["CMD", "echo", "''", ">", "/dev/tcp/127.0.0.1/9300", "||", "exit", "1"]
      retries: 30
      interval: 10s
      timeout: 10s
{% endfor %}

  redis-sentinel:
    image: {{ richie_redis_sentinel_docker_image }}
    environment:
      REDIS_MASTER_HOST: {{ richie_docker_deploy_stack_name }}_{{ richie_redis_primary_container }}
      # Accept host names
      REDIS_SENTINEL_RESOLVE_HOSTNAMES: "yes"
      # Enabling the announce-hostnames global configuration makes Sentinel use host names instead. 
      # This affects replies to clients, values written in configuration files, 
      # the REPLICAOF command issued to replicas, etc.
      REDIS_SENTINEL_ANNOUNCE_HOSTNAMES: "yes"
      # announce the hostname of the container
      # REDIS_SENTINEL_ANNOUNCE_IP:
    deploy:
      replicas: {{ richie_redis_sentinel_replicas }}
      # Use DNS Round Robin to bypass the docker routing mesh and to allow HAProxy parse the DNS
      # entries to discover all the redis sentinel instance replicas.
      endpoint_mode: dnsrr
      mode: replicated
      restart_policy:
        condition: on-failure
      placement:
        max_replicas_per_node: {{ ( richie_redis_sentinel_replicas / ( docker_swarm_cluster_group_length | int ) ) | round(0, 'ceil') | int }}

{% for redis_container_name, redis_container in richie_redis_containers.items() %}

  {{ redis_container_name }}:
    image: {{ richie_redis_docker_image }}
    environment:
      ALLOW_EMPTY_PASSWORD: "yes"
      REDIS_MASTER_HOST: {{ richie_docker_deploy_stack_name }}_{{ richie_redis_primary_container }}
      REDIS_REPLICATION_MODE: {{ redis_container.mode }}
      # announce the hostname of the container, because the IP address can change after a container restart
      REDIS_REPLICA_IP: {{ richie_docker_deploy_stack_name }}_{{ redis_container_name }}
    healthcheck:
      test: ["CMD-SHELL", "redis-cli ping | grep PONG"]
      interval: 5s
      timeout: 3s
      retries: 30
    deploy:
      replicas: 1
      mode: replicated
      restart_policy:
        condition: on-failure
      resources:
        limits:
          memory: 4G
      placement:
        max_replicas_per_node: 1
        constraints:
          - node.labels.fqdn == {{ redis_container.host_fqdn }}
{% endfor %}

{{ helper.configs() }}
{{ helper.secrets() }}
